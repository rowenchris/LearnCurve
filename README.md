# LearnCurve ğŸ“ˆ

**An interactive visualization of how neural networks learn to approximate functions**

Watch a neural network learn in real-time through a structured three-phase workflow. No installation required â€“ runs entirely in your browser!

## ğŸ¯ Try It Now

**[Launch LearnCurve â†’](https://rowenchris.github.io/LearnCurve/)**

---

## Three-Phase Learning Workflow

LearnCurve guides you through the machine learning process:

| Phase | What You Do |
|-------|-------------|
| **â‘  Create Training Data** | Define a "data recipe" function, set the range and noise level, then generate training examples |
| **â‘¡ Train the Model** | Configure network architecture and hyperparameters, then watch the model learn |
| **â‘¢ Evaluate & Compare** | Test on held-out data to measure generalization; save and compare different runs |

---

## What You'll See

| Panel | Description |
|-------|-------------|
| **Network Design** | Animated diagram showing neurons, weights, and biases with operation counts |
| **Examples vs Model** | Training points (blue) vs model prediction (blue curve), with optional held-out data and recipe overlay |
| **Training Trace** | Error curves (training + test) with rolling average and Î”Error indicator |
| **Loss Landscape** | 2D heatmap showing loss surface with training path overlay |

**Goal:** Train the network until the blue prediction curve matches the data!

---

## âœ¨ Features

### ğŸ® Data Generation (Phase 1)
- **Data Recipe**: Any math expression (`sin(x)`, `x^2`, `x^3-3*x`, `abs(x)`, etc.)
- **Range Control**: Set x-range for data generation
- **Noise Level**: Add realistic noise to training data
- **Train/Test Split**: Automatic 80/20 split with held-out test set

### ğŸ§  Network Design & Training (Phase 2)
- Adjust hidden layers (1-5) and neurons per layer (1-32)
- Unified slider controls with labeled value displays
- **Learning Rate (Î·)**: Control step size (0.001 to 0.5)
- **Optimizer**: Compare Simple (SGD) vs Adam
- **Activation (Ïƒ)**: Choose between ReLU and Sigmoid
- **Training Range**: Restrict training to test extrapolation
- Watch weights and biases update in real-time

### ğŸ“Š Visualization
- **Examples vs Model**: Prediction curve with training points; reveal held-out test data and original recipe
- **Training Trace**: Error over time with test error overlay and zoom control
- **Loss Landscape**: 2D heatmap showing the loss surface as a function of two selected weights
- **Equations Panel**: Forward and backward pass math explained

### ğŸ—ºï¸ Loss Landscape Algorithm
The loss landscape is computed using an **exploration-based algorithm**:
- **150 training runs** from a 15Ã—10 grid of starting positions
- Each run trains for **600 steps** with all weights free to move
- **5Ã—5 patches** are probed around start and end positions
- Results are **averaged** where runs overlap, reducing path-dependency noise
- Final **60Ã—40 display grid** (2,400 cells) captures real structure
- Computes in **~2-3 seconds** with no intermediate UI updates

### ğŸ”¬ Evaluation & Comparison (Phase 3)
- Evaluate model on held-out test data
- See in-range vs extrapolation performance
- Save two training runs (A & B) with different settings
- Compare learning curves side-by-side

### ğŸ“– Learning Guide
- Built-in sidebar explaining key ML concepts
- "The Big Picture" overview
- Key Terms glossary
- Math explanations (forward pass, backprop, chain rule)
- Suggested experiments

### ğŸ“¸ Capture
- **Screenshot**: Download PNG of current state
- **Animated GIF**: Record training convergence

---

## ğŸ“ Mathematical Notation

LearnCurve uses student-friendly notation aligned with standard machine learning education:

| Symbol | Meaning |
|--------|---------|
| `x` | Input value |
| `t` | Target (true value from data recipe) |
| `y` | Model prediction |
| `w_ij` | Weights to hidden layer (layer i, neuron j) |
| `w_j` | Weights to output (neuron j) |
| `b_ij` | Hidden layer biases (layer i, neuron j) |
| `b` | Output bias |
| `h_ij` | Hidden neuron activations (layer i, neuron j) |
| `z_ij` | Pre-activation (weighted sum) |
| `Ïƒ` | Activation function (ReLU or Sigmoid) |
| `E` | Error: E = Â½(y âˆ’ t)Â² |
| `Î·` | Learning rate |

### Forward Pass
```
z_ij = w_ijÂ·x + b_ij    (weighted sum, hidden layer)
h_ij = Ïƒ(z_ij)          (activation)
y = wâ‚hâ‚ + wâ‚‚hâ‚‚ + ... + b  (output)
E = Â½(y âˆ’ t)Â²           (error)
```

### Backward Pass (Gradients)
```
âˆ‚E/âˆ‚y = y âˆ’ t           (error derivative)
âˆ‚E/âˆ‚w_j = (âˆ‚E/âˆ‚y)Â·h_j   (output weight gradient)
âˆ‚E/âˆ‚h_ij = (âˆ‚E/âˆ‚y)Â·w_j  (hidden activation gradient)
âˆ‚E/âˆ‚z_ij = (âˆ‚E/âˆ‚h_ij)Â·Ïƒ'(z_ij)  (chain rule)
```

### Weight Update (SGD)
```
w â† w âˆ’ Î· Â· âˆ‚E/âˆ‚w
```

---

## ğŸ“ Who Is This For?

Students learning machine learning who have:
- âœ… Basic algebra and function notation
- âœ… Familiarity with calculus concepts (derivatives, chain rule)
- âœ… Curiosity about how AI learns!

### Concepts Demonstrated
1. **Training vs Test Data** â€“ Why we hold out data for evaluation
2. **Forward Pass** â€“ How inputs flow through layers (weighted sums + activations)
3. **Error Function** â€“ Measuring prediction error (MSE): E = Â½(y âˆ’ t)Â²
4. **Backpropagation** â€“ Computing gradients via chain rule
5. **Gradient Descent** â€“ Adjusting weights to minimize error
6. **Epochs** â€“ Multiple passes through training data
7. **Overfitting** â€“ When models memorize instead of generalize
8. **Interpolation vs Extrapolation** â€“ Performance inside vs outside training range

---

## ğŸ§ª Suggested Experiments

| Experiment | What You'll Learn |
|------------|-------------------|
| Train on `sin(x)` | Networks can learn periodic functions |
| Try `x^3-3*x` | Polynomial with turning points |
| Compare `abs(x)` vs `sign(x)` | Sharp features are harder |
| Add noise (0.1) | Robustness to noisy data |
| Compare Adam vs Simple | Why adaptive optimizers help |
| 1 layer vs 4 layers | Depth vs training speed trade-off |
| 3 neurons vs 16 neurons | Width vs expressiveness |
| Restrict training range | See extrapolation failures |
| Watch Loss Landscape | See gradient descent navigate the loss surface |

---

## ğŸš€ Running Locally

### Quick Start
Just open `index.html` in your browser!

### With Local Server (for GIF capture)
```bash
cd MLdemo
python3 -m http.server 8000
# Open http://localhost:8000
```

---

## ğŸ› ï¸ Technical Details

- **Pure HTML/CSS/JavaScript** â€“ zero dependencies
- **No build step** â€“ single self-contained file
- **~8000 lines** of code
- **Responsive design** â€“ works on desktop (sidebar hidden on mobile)
- **Gradient clipping** â€“ numerically stable training
- **Smart auto-pause** â€“ stops training when loss improvement drops below 1% over 1000 steps
- **Optimized loss surface** â€“ exploration algorithm with probe averaging for consistent visualization

---

## ğŸ¤ Credits

Built by **Chris Rowen** with assistance from **Claude** (Anthropic)

---

## ğŸ“„ License

[MIT License](LICENSE) â€“ Use, modify, and share freely!

Copyright Â© 2025 Chris Rowen

---

â­ **Star this repo if you find it useful for teaching or learning!**
